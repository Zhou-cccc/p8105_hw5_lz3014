p8105_hw5_lz3014
================
2024-11-15

``` r
library(tidyverse)
library(broom)
library(purrr)

set.seed(1115)
```

# Problem 1

### 1. Write a function to simulate and check birthdays

``` r
birthd_sim <- function(n){
  bdays = sample(1:365, n, replace = TRUE)
  duplicate = (length(unique(bdays)) < n)
  
  return(duplicate)
}
```

### 2. Run this function 10000 times for each group size between 2 and 50

``` r
dup_bd_prob <- expand_grid(
  n = 2:50,
  iter = 1:10000
) |>
  mutate(dup = map_lgl(n, birthd_sim)) |>
  group_by(n) |>
  summarize(dup_prob = mean(dup))

head(dup_bd_prob, 10)
```

    ## # A tibble: 10 × 2
    ##        n dup_prob
    ##    <int>    <dbl>
    ##  1     2   0.0017
    ##  2     3   0.0088
    ##  3     4   0.015 
    ##  4     5   0.0237
    ##  5     6   0.0367
    ##  6     7   0.0544
    ##  7     8   0.0752
    ##  8     9   0.0986
    ##  9    10   0.117 
    ## 10    11   0.137

### 3. Plot: Probability of Shared Birthday

``` r
dup_bd_prob |>
  ggplot(aes(x = n, y = dup_prob)) +
  geom_line() +
  labs(
    title = "Probability of Shared Birthday",
    x = "Group Size",
    y = "Probability of Shared Birthday"
  )
```

![](p8105_hw5_lz3014_files/figure-gfm/unnamed-chunk-4-1.png)<!-- -->

**Comments:**  
- The curve indicates a rapid increase in the probability of shared
birthdays as the group size grows, and then reach near 100% when n is
around 40.

# Problem 2

### 1. $\mu$ = 0

``` r
mu_0 <- tibble(
  iter = 1:5000,
  data = map(iter, \(i) rnorm(n = 30, mean = 0, sd = 5))
) |>
  mutate(
    t_test = map(data, \(x) t.test(x, mu = 0, conf.level = 0.95)),
    t_result = map(t_test, broom::tidy)) |>
  unnest(t_result) |>
  select(iter, data, mu_estimate = estimate, p_value = p.value)

head(mu_0, 10)
```

    ## # A tibble: 10 × 4
    ##     iter data       mu_estimate p_value
    ##    <int> <list>           <dbl>   <dbl>
    ##  1     1 <dbl [30]>      0.0899  0.934 
    ##  2     2 <dbl [30]>      0.202   0.843 
    ##  3     3 <dbl [30]>      1.55    0.0889
    ##  4     4 <dbl [30]>      0.0111  0.986 
    ##  5     5 <dbl [30]>     -0.0320  0.972 
    ##  6     6 <dbl [30]>     -0.478   0.558 
    ##  7     7 <dbl [30]>      0.524   0.497 
    ##  8     8 <dbl [30]>     -0.501   0.631 
    ##  9     9 <dbl [30]>     -0.543   0.543 
    ## 10    10 <dbl [30]>      0.542   0.597

Write it into a function and repeat the above for μ={1,2,3,4,5,6}

``` r
t_test_func <- function(mu, sample_size = 30, sigma = 5, alpha = 0.05){
  tibble(
  iter = 1:5000,
  data = map(iter, \(i) rnorm(n = sample_size, mean = mu, sd = sigma))
) |>
  mutate(
    t_test = map(data, \(x) t.test(x, mu = 0, conf.level = (1 - alpha))),
    t_result = map(t_test, broom::tidy)) |>
  unnest(t_result) |>
  select(iter, data, mu_estimate = estimate, p_value = p.value)
}
```

``` r
sim_6_mu <- map_df(0:6, \(mu) t_test_func(mu) 
                      |> mutate(mu_true = mu)) |>
  select(-iter)

head(sim_6_mu, 10)
```

    ## # A tibble: 10 × 4
    ##    data       mu_estimate p_value mu_true
    ##    <list>           <dbl>   <dbl>   <int>
    ##  1 <dbl [30]>       0.423   0.653       0
    ##  2 <dbl [30]>      -1.48    0.132       0
    ##  3 <dbl [30]>       0.886   0.236       0
    ##  4 <dbl [30]>       0.273   0.795       0
    ##  5 <dbl [30]>      -0.428   0.589       0
    ##  6 <dbl [30]>       0.464   0.590       0
    ##  7 <dbl [30]>      -0.695   0.448       0
    ##  8 <dbl [30]>      -0.375   0.682       0
    ##  9 <dbl [30]>      -0.794   0.307       0
    ## 10 <dbl [30]>      -0.207   0.841       0

### 2. Plot: Power V.S. True Mean

``` r
sim_6_mu_power <- sim_6_mu |>
  group_by(mu_true) |>
  summarize(
    reject_count = sum(p_value < 0.05),
    total_count = n(),
    power = reject_count / total_count
  )
```

``` r
sim_6_mu_power |>
  ggplot(aes(x = mu_true, y = power)) +
  geom_line() +
  geom_point() +
  labs(
    title = "Power V.S. True Mean",
    x = "True Mean (μ)",
    y = "Power"
  ) +
  theme_minimal()
```

![](p8105_hw5_lz3014_files/figure-gfm/unnamed-chunk-9-1.png)<!-- -->

**Association between effect size and power**  
- As the true mean (effect size) increases, the power of the test also
increases because it’s easier to notice the difference when the true
mean is higher.

### 3. `Plot1: Average Estimate of $\mu$ V.S. True Mean` and `Plot2: Average Estimate of $\mu$ for Rejected Nulls`

Plot1:

``` r
avg_mu_estimate <- sim_6_mu |>
  group_by(mu_true) |>
  summarize(mean_mu_estimate = mean(mu_estimate))
```

``` r
avg_mu_estimate |>
  ggplot(aes(x = mu_true, y = mean_mu_estimate)) +
  geom_line() +
  geom_point() +
  labs(
    title = "Average Estimate of μ V.S. True Mean",
    x = "True Mean (μ)",
    y = "Average Estimate (μ_hat)"
  ) +
  theme_minimal()
```

![](p8105_hw5_lz3014_files/figure-gfm/unnamed-chunk-11-1.png)<!-- -->

Plot2:

``` r
avg_mu_estimate_rejected <- sim_6_mu |>
  filter(p_value < 0.05) |>
  group_by(mu_true) |>
  summarize(mean_mu_estimate_rejected = mean(mu_estimate))
```

``` r
avg_mu_estimate_rejected |>
  ggplot(aes(x = mu_true, y = mean_mu_estimate_rejected)) +
  geom_line() +
  geom_point() +
  labs(
    title = "Average Estimate of μ for Rejected Nulls",
    x = "True Mean (μ)",
    y = "Average Estimate (μ_hat) for Rejected Nulls"
  ) +
  theme_minimal()
```

![](p8105_hw5_lz3014_files/figure-gfm/unnamed-chunk-13-1.png)<!-- -->

Is the sample average of $\hat{\mu}$ across tests for which the null is
rejected approximately equal to the true value of $\mu$ ? Why or why
not?  
- No. Because the tests for which the null is rejected are biased
towards samples with extreme values, which tend to have larger
estimation compared to the true mean.

# Problem 3

``` r
homicide_data <- read_csv("./data/homicide-data.csv")
```

    ## Rows: 52179 Columns: 12
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## chr (9): uid, victim_last, victim_first, victim_race, victim_age, victim_sex...
    ## dbl (3): reported_date, lat, lon
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

``` r
head(homicide_data, 10)
```

    ## # A tibble: 10 × 12
    ##    uid        reported_date victim_last  victim_first victim_race victim_age
    ##    <chr>              <dbl> <chr>        <chr>        <chr>       <chr>     
    ##  1 Alb-000001      20100504 GARCIA       JUAN         Hispanic    78        
    ##  2 Alb-000002      20100216 MONTOYA      CAMERON      Hispanic    17        
    ##  3 Alb-000003      20100601 SATTERFIELD  VIVIANA      White       15        
    ##  4 Alb-000004      20100101 MENDIOLA     CARLOS       Hispanic    32        
    ##  5 Alb-000005      20100102 MULA         VIVIAN       White       72        
    ##  6 Alb-000006      20100126 BOOK         GERALDINE    White       91        
    ##  7 Alb-000007      20100127 MALDONADO    DAVID        Hispanic    52        
    ##  8 Alb-000008      20100127 MALDONADO    CONNIE       Hispanic    52        
    ##  9 Alb-000009      20100130 MARTIN-LEYVA GUSTAVO      White       56        
    ## 10 Alb-000010      20100210 HERRERA      ISRAEL       Hispanic    43        
    ## # ℹ 6 more variables: victim_sex <chr>, city <chr>, state <chr>, lat <dbl>,
    ## #   lon <dbl>, disposition <chr>

**Description**  
- This dataset contains 52179 rows and 12 columns, each row representing
a homicide case in the United States. Variables include victim details
(e.g. name, race, age), case location (e.g. city, state, latitude,
longitude), and case status (e.g. closed, open).

### 1. create city_state variable and summarize

``` r
homicide_num <- homicide_data |>
  mutate(city_state = str_c(city, state, sep = ", ")) |>
  group_by(city_state) |>
  summarize(
    total_cases = n(),
    unsolved_cases = sum(disposition %in% c("Closed without arrest", "Open/No arrest")),
    .groups = "drop"
  )
```

### 2. Prop.tes for city of Baltimore, MD

``` r
baltimore_num <- homicide_num |>
  filter(city_state == "Baltimore, MD") 

baltimore_unso_prop <- prop.test(baltimore_num$unsolved_cases, baltimore_num$total_cases) |>
  broom::tidy()

baltimore_pull <- baltimore_unso_prop |> 
  select(estimate, conf.low, conf.high) |>
  knitr::kable()

baltimore_pull
```

|  estimate |  conf.low | conf.high |
|----------:|----------:|----------:|
| 0.6455607 | 0.6275625 | 0.6631599 |

### 3. Prop.test for each of the cities

``` r
homicide_prop <- homicide_num |>
  mutate(
    prop_test = map2(unsolved_cases, total_cases, \(x, y) prop.test(x, y)),
    tidy_result = map(prop_test, broom::tidy)
  ) |>
  unnest(tidy_result) |>
  select(city_state, estimate, conf.low, conf.high)

head(homicide_prop, 10)
```

    ## # A tibble: 10 × 4
    ##    city_state      estimate conf.low conf.high
    ##    <chr>              <dbl>    <dbl>     <dbl>
    ##  1 Albuquerque, NM    0.386    0.337     0.438
    ##  2 Atlanta, GA        0.383    0.353     0.415
    ##  3 Baltimore, MD      0.646    0.628     0.663
    ##  4 Baton Rouge, LA    0.462    0.414     0.511
    ##  5 Birmingham, AL     0.434    0.399     0.469
    ##  6 Boston, MA         0.505    0.465     0.545
    ##  7 Buffalo, NY        0.612    0.569     0.654
    ##  8 Charlotte, NC      0.300    0.266     0.336
    ##  9 Chicago, IL        0.736    0.724     0.747
    ## 10 Cincinnati, OH     0.445    0.408     0.483

### 4. Plot: Proportion of Unsolved Homicides by City

``` r
homicide_prop |>
  ggplot(aes(x = reorder(city_state, estimate), y = estimate)) +
  geom_point() +
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high)) +
  labs(
    title = "Proportion of Unsolved Homicides by City",
    y = "Estimated Proportion of Unsolved Homicides",
    x = "City"
  ) +
  theme_minimal() +
  theme(
    axis.text.x = element_text(angle = 90, hjust = 1)
  )
```

![](p8105_hw5_lz3014_files/figure-gfm/unnamed-chunk-18-1.png)<!-- -->
